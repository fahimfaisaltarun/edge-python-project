{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AUKPJYvV1L_O"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIgQPYSe1Ty2",
    "outputId": "f1d79742-4e9d-4b1e-d4e4-5200595b6b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kDcRfQU144C",
    "outputId": "6afbbd5f-e8ba-478e-e6a9-c373f36155dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Padded train shape: (65816, 185)\n",
      "Padded test shape: (16454, 185)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_excel('/content/gdrive/My Drive/Colab Notebooks/all_data.xlsx')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "df=df.dropna(subset=['Cleaned_Text'])\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Cleaned_Text'], df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess data for MNB (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Preprocess data for CNN and ANN (Tokenization and Padding)\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
    "tokenizer.fit_on_texts(df['Cleaned_Text'])\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max(len(x) for x in X_train_tokens)  # Get max sequence length\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=max_length)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode target variables\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "# Ensure the input is numpy array format\n",
    "X_train_padded = np.array(X_train_padded)\n",
    "X_test_padded = np.array(X_test_padded)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "# Check unique values\n",
    "print(np.unique(y_train))\n",
    "\n",
    "# Use assertions to ensure no non-numeric data slips through\n",
    "assert np.all(np.isfinite(X_train_padded)), \"X_train_padded contains non-numeric data\"\n",
    "assert np.all(np.isin(y_train, [0, 1])), \"y_train contains values other than 0 and 1\"\n",
    "\n",
    "\n",
    "# For CNN and ANN\n",
    "max_length = max([len(x) for x in tokenizer.texts_to_sequences(df['Cleaned_Text'])])  # Global max length\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=max_length)\n",
    "\n",
    "\n",
    "def create_cnn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Embedding(5000, 50, input_length=input_dim),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# For CNN and ANN, ensure they're receiving the same type of processed data:\n",
    "X_train_padded = np.array(X_train_padded).astype(np.float32)\n",
    "X_test_padded = np.array(X_test_padded).astype(np.float32)\n",
    "\n",
    "# Check dimensions\n",
    "print(\"Padded train shape:\", X_train_padded.shape)\n",
    "print(\"Padded test shape:\", X_test_padded.shape)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, build_fn, epochs=10, batch_size=32, verbose=0):\n",
    "        self.build_fn = build_fn\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = build_fn()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        return (predictions > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        # Return probabilities for both classes\n",
    "        return np.hstack([1-predictions, predictions])\n",
    "\n",
    "\n",
    "cnn = KerasClassifierWrapper(build_fn=lambda: create_cnn_model(max_length), epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Ensure the custom TransformerBlock imports are correct\n",
    "from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, GlobalMaxPooling1D, Dense, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "class SimpleTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0):\n",
    "        super(SimpleTransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Dense(ff_dim, activation=\"relu\")\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_very_simple_tnn_model(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    embedding_layer = Embedding(5000, 10, input_length=input_dim)(input_layer)  # Further reduced embedding dimension\n",
    "    transformer_block = SimpleTransformerBlock(10, 1, 10)  # Reduced complexity\n",
    "    x = transformer_block(embedding_layer)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Wrap the even simpler model in KerasClassifierWrapper\n",
    "tnn= KerasClassifierWrapper(build_fn=lambda: create_very_simple_tnn_model(max_length), epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0MIOcRB2Uce",
    "outputId": "fc85415f-555b-44d7-e5b0-9663a204795a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 6s 13ms/step\n",
      "412/412 [==============================] - 4s 10ms/step\n",
      "412/412 [==============================] - 4s 9ms/step\n",
      "412/412 [==============================] - 4s 10ms/step\n",
      "412/412 [==============================] - 4s 10ms/step\n",
      "515/515 [==============================] - 6s 12ms/step\n",
      "Accuracy of stacked model: 0.770754831651878\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Define stacking ensemble with the updated setup\n",
    "# Define the stacking ensemble\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('RF', RandomForestClassifier(n_estimators=100)),  # Example model; use your CNN model\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('cnn', cnn),\n",
    "\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(),\n",
    "    stack_method='predict_proba',\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# Fit stacked model\n",
    "stack_model.fit(X_train_padded, y_train)  # Ensure X_train_padded is appropriate for all models\n",
    "y_pred = stack_model.predict(X_test_padded)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of stacked model: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
